---
title: "day1_complete"
output: html_document
---

# Agenda  
Today we are exploring the following topics:

- What is and when to use multivariate analysis?
- Different types of multivariate analysis
- Import a soybean yield dataset with multiple variables to be analyzed
- Understand what is multicollinearity, its effects, implications, and remedies  
- Run principal component analysis (PCA) for dimensionality reduction
- Run k-means for clustering


# Intro 
## What is multivariate analysis?  

Most common: multiple Xs (predictors)  
    y ~ x1 + x2 + .... xn
    yield ~ precip + temp + som + hybrid
    
Perhaps multiple Xs (predictors) without a Y (outcome)
    ~ x1 + x2 + .... xn

While some can have multiple Ys (outcomes)
  y1 + y2 ~ x1 + x2 (MANOVA)
  yield + biomass ~ precip + temp  
  
## When to use multivariate?  
When multiple predictors can be used to explain an outcome.  
When multiple predictors can be used to create data-driven groups (clusters).  

Commonly not a "designed experiment" analysis, but an exploratory relationship approach.

## Main types of multivariate  
- Dimensionality reduction  
    - **Principal component analysis (PCA)**
    - Principal coordinate analysis (PCoA)
    - Factor analysis
    
- Clustering 
    - **k-means**, c-means, x-means  
    - **Non-numerical multidimensional scaling**
    - nearest neighbor
  
- Relationship between variables / Prediction  
  - Multiple linear regression/stepwise regression
  - **Random forest**
  - **Conditional inference tree**

## A few ways to characterize multivariate analysis  
- Based on analysis goal:
    - Decrease number of dimensions  
    - Create clusters  
    - Establish significant relationships among multiple predictors and an outcome.  

- Based on Y (outcome) existence:
    - Supervised
    - Unsupervised  

- If Y exists, based on its type:
    - Categorical: classification
    - Numerical: regression

# 1. Setup  
```{r Installing and loading packages, warning=F}
#install.packages("easypackages")
library(easypackages)

packages("janitor") # clean column names
packages("tidyverse") # wrangling and general plotting
packages("ggcorrplot") # exploratory correlation plot
packages("broom") # extracting model info as data frame
packages("car") # vif
packages("factoextra") # multivariate plotting

# Setting plotting theme
theme_set(theme_bw()+
            theme(panel.background = element_rect(fill="gray80"),
                  panel.grid = element_blank(),
                  strip.text = element_text(face = "bold", size=10)))


```

```{r mvadf}
mvadf <- read_csv()
```

# 2. Wrangling  
```{r print mvadf}
mvadf
```
The data set contains categorical and numerical columns.

```{r mvadf summary}
mvadf %>%
  summary()
```
Let's transform character variables into factor, 

AND

select some numerical variables to continue our analysis (precip, temperature, radiation, and vapor pressure deficit).  

```{r mvadf_w}
mvadf_w <- mvadf %>%
  clean_names() %>%
  mutate(across(where(is.character), factor)) %>%
  dplyr::select(gy:sand, starts_with(c("pp_","tm_","rad_","vpd_")))

mvadf_w
```

# 3. EDA  
How are variables related? Any strong correlations that we should watch out for?  
```{r correlation matrix}
# Estimating significance
p.mat <-  mvadf_w %>%
  dplyr::select(where(is.numeric)) %>%
  cor_pmat()

mvadf_w %>%
  dplyr::select(where(is.numeric)) %>%
  cor() %>%
  ggcorrplot(hc.order = TRUE,
             type = "lower", 
             p.mat = p.mat, 
             insig = "blank",
             lab= TRUE)+
  ggsave("../output/cormat.png", width = 10, height = 10)


```

How do variables relate to grain yield in a bivariate relationship?  

```{r eda2}
mvadf_w %>%
  dplyr::select(where(is.numeric)) %>%
  pivot_longer(cols=!gy) %>%
  ggplot(aes(x=value, y=gy))+
  geom_point(shape=21, alpha=.7, fill="purple")+
  geom_smooth(method="lm", se=F, color="yellow", size=3)+
  facet_wrap(~name, scales = "free_x", ncol=6)+
  ggsave("../output/gyvsall.png", width = 15, height = 15)



```

# 4. Multicollinearity 
## Concepts  
Multicollinearity definition: more than two explanatory variables in a multiple regression model are highly linearly related.


Multicollinearity is an issue because:  
  - Model estimates magnitude and direction (+ or -) can change for multicollinear variables compared to a non-multicollinear model.  
  
  - Model estimates standard error are inflated, directly affecting p-values, estimate significance, and power.

## Applied example  
Let's select a few variables to run some tests.  
Two uncorrelated variables:
```{r uncorrelated}
mvadf_w %>%
  ggplot(aes(x=, y=))+
  geom_point()+
  geom_smooth(method="lm")
```

Two correlated variables:
```{r correlated}
mvadf_w %>%
  ggplot(aes(x=, y=))+
  geom_point()+
  geom_smooth(method="lm")

```

Now let's fit some models with one or two uncorrelated and correlated variables explaining yield and see what happens.
```{r gy ~ vpd_pr}
lm_vpd 

lm_vpd

```

```{r gy ~ tm_pre}
lm_tm <- lm(gy ~ tm_pre,
   data=mvadf_w) %>%
  tidy() %>%
  filter(term!="(Intercept)") %>%
  mutate(mod="lm_tm")

lm_tm
```

```{r gy ~ vpd_pre + rad_2}
lm_vpd.rad 

lm_vpd.rad

```

```{r gy ~ vpd_pre + tm_pre}
lm_vpd.tm <- lm(gy ~ vpd_pre + tm_pre,
   data=mvadf_w) %>%
  tidy() %>%
  filter(term!="(Intercept)") %>%
  mutate(mod="lm_2var.cor")

lm_vpd.tm

```

```{r Checking multicollinearity}
lm_vpd %>%
  bind_rows(lm_tm, 
            lm_vpd.rad,
            lm_vpd.tm) %>%
  mutate(mod=factor(mod,
                    levels=c("lm_vpd",
                             "lm_tm",
                             "lm_2var.uncor",
                             "lm_2var.cor"))) %>%
    filter(term!="rad_2") %>%
  ggplot(aes(x=mod))+
  geom_pointrange(aes(y=estimate,
    ymin=estimate-std.error,
                      ymax=estimate+std.error))+
  scale_x_discrete(guide = guide_axis(n.dodge = 2))+
  facet_wrap(~term, scales = "free_y")

```

What has happened with tm_pre and vpd_pre estimates and standard error when modeled i) alone, or with another uncorrelated variable vs. ii) with another correlated variable?

Let's check the variance inflation factor (VIF) of both uncorrelated and correlated models
```{r vif}
# Uncorrelated
vif(lm(gy ~ vpd_pre + rad_2,
   data=mvadf_w)) 

# Correlated
vif(lm(gy ~ vpd_pre + tm_pre,
   data=mvadf_w))

```
VIF values range from 1 to positive infinite.
General rule of thumb:
    - VIF ~ 1: no multicollinearity
    - VIF between 1 and 5: moderate multicollinearity
    - VIF > 5: severe multicollinearity

## Dealing with multicollinearity  
So what now?  
How can we deal with correlated variables in a multivariate approach?  
Options:  
    - Dimensionality reduction  
        - By hand  
        - By math  
  
    - Algorithm that handles multicollinearity
        - Variable selection  
        - Multivariate by ensembling multiple bivariates  

Many multivariate approaches deal with some sort of similarity/dissimilarity measure among predictors.  

In those cases, predictors with vastly different scales (e.g. SOM from 0 to 7 vs precipitation from 0 to 300) need to be normalized so measurement scale does not affect variable importance.    

Thus, our numerical predictor variables need to be normalized (center and scale) before starting our multivariate analysis. 

Some analysis do the normalization for you (like PCA), and other don't (like k-means), so need to be aware of this to ensure data is normalized.

Since both PCA and k-means only take numerical variables, let's select them now.  
```{r selecting only numerical vars}
mvadf_wn 

mvadf_wn
```

# 5. PCA  
PCA is a dimensionality reduction approach that accomodates only numerical variables. 

Finds linear relationships among predictors that can be represented in a lower number of uncorrelated dimensions.

Works well when at least some predictors are correlated.  

PCA:
- Is used for dimensionality reduction 
- Is an unsupervised analysis (no outcome)
- Only takes predictors
- Predictors need to be numerical

```{r pca in action}
knitr::include_graphics("https://builtin.com/sites/default/files/inline-images/Principal%20Component%20Analysis%20second%20principal.gif")
```

```{r pca model}
mod_pca 
```

```{r pca checking number of components}
# Scree plot


```
PCs 1 and 2 explain ~27% and ~14% (41%) of total variance.
Not great, indicates that original variables were not as highly correlated.


If wanted to use enough PCs to explain 60% of total variance, how many would we need?
```{r PCs to explain 60% variance}
mod_pca %>%
  get_eig() %>%
  mutate(pc=1:nrow(.)) %>%
  ggplot(aes(x=pc, y=cumulative.variance.percent))+
  geom_bar(stat="identity")+
  geom_hline(yintercept = 60)


```
We would need 5 PCs.
Normally we wish to use 2-3 PCs, but 5 is certainly better than 24 original variables.  

Let's inspect PC1.
What are the weights that each variable received in this PC?
```{r PC1 weights}
mod_pca$rotation %>%
  as.data.frame() %>%
  rownames_to_column(var = "var") %>%
  ggplot(aes(x=reorder(var,desc(PC1)), y=PC1))+
  geom_bar(stat="identity", aes(fill=PC1), show.legend = F)+
  scale_fill_gradient(low = "red", high = "blue")+
  theme(axis.text.x = element_text(angle=45, hjust=1))

```

Which variables contributed most to PC1, regardless of direction?
```{r variables contributing to PC1}

```
Let's the eigenvectors for both PCs 1 and 2 variables:
```{r pca variable contribution }
fviz_pca_var(mod_pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )

```
The longer is the eigenvector for a given variable, the more important it is towards that PC.  

Let's plot PC1 vs PC2 scores, look for any groupings.
```{r PC1 vs PC2}


```
No clear groups.

What if we draw ellipses by fungicide levels?
```{r PC1 vs PC2 by fungicide}
fviz_pca_ind(mod_pca,
             col.ind = factor(mvadf_w$fung),
             addEllipses = TRUE)

```
What did we learn?
- Original variables not strongly correlated
- Need at least 5 PCs to cover ~60% of original variables variance
- Most important variables in PC1 were related to vpd and temperature.

What now?
Let's add the first 5 PCs to our original dataset and run a regression.
```{r pca scores}
# Extract first 5 PCs scores
pca_scores 

pca_scores
```

```{r pca regression}
# Adding PCs 1-5 scores to original data set
mvadf_wpostpca 

# Regression of yield ~ PCs

lm_pca 

# Summary  
summary(lm_pca)

# Checking VIF
vif(lm_pca)
```

```{r pca regression plots}
# Plotting yield vs PC1


# Plotting yield vs PC3


```

Only PCs 1, 3, and 5 explained yield (look at which variables were most important to each PC for interpretation).

As expected, PCs were not multicollinear (VIF=1).

# 6. k-means  
k-means is a clustering algorithm and partitions the data into k groups, where k is defined by the user. 

k-means works by 
- randomly choosing k samples from our data to be the initial cluster centers, 
- calculates the distance of all observations to the clusters centers, 
- assigns a cluster class to each observation based on closest distance
- using all members of a cluster, recalculates cluster mean
- repeats the entire process until cluster means stabilize
```{r k-means in action}
knitr::include_graphics("https://miro.medium.com/max/960/1*KrcZK0xYgTa4qFrVr0fO2w.gif")

```

k-means:
- Is used for clustering
- Is an unsupervised analysis (no outcome)
- Only takes predictors
- Predictors need to be numerical


k-means is useful when clusters are circular, but can fail badly when clusters have odd shapes or outliers.
```{r clustering algorithms comparison}
knitr::include_graphics("https://miro.medium.com/max/1400/1*oNt9G9UpVhtyFLDBwEMf8Q.png")

```

k-means does not normalize our data for us like PCA did, so we will need to do that before running the model.

Also, we need to define the number of clusters we want. 
Any thoughts? 
Let's try 4.
```{r kmeans model }
# normalizing the data
mvadf_wnn <- mvadf_wn %>%
  mutate(across(where(is.numeric), ~scale(.x))) 

mod_km

mod_km
```

Since the choice of k can be subjective, we will need to find an objective way to select the value of k that most properly represents our dataset.

```{r choosing k}
# Total error x k


# Gap-stat x k


# Silhouette width

```
gap-stat: k=8
silhouette: k=2

```{r mod_km2 }
mod_km2 <- kmeans(mvadf_wnn, 
                 centers= 2,
                 nstart=10)

mod_km2
```

Now how can we visually inspect the resutls of k-means?
We can either 
- add the cluster column to original dataset and explore the distribution of each variable agains cluster id, OR  

- use a function that summarises all the original variables into PCs and plots the cluster ids.

```{r cluster x variable boxplots}
mvadf_wn %>%
  add_column(gy=mvadf_w$gy) %>%
  mutate(cluster=mod_km2$cluster,
         cluster=factor(cluster)) %>%
  pivot_longer(!cluster) %>%
  ggplot(aes(x=cluster, y=value, color=cluster))+
    geom_boxplot(show.legend = F)+
  facet_wrap(~name, scales="free_y", ncol=6)
  

```
We could actually run ANOVA models for each original variable of the form  

              var ~ cluster, for ex. clay ~ cluster  
              
and extract cluster mean and pairwise comparison to understand what variables had significant differences among clusters.

```{r kmeans PCA plot}


```
Notice how, behind the scenes, the fviz_cluster function ran a PCA and is showing us a plot with PCs 1 and 2 on the axis (same result as we obtained on the PCA analysis).  

# Exercise  
What if we had chose k=8? How would that have changed the plots above?
Go ahead and try yourself!

# Summary  
Today, we covered:

- When multivariate analysis can be used  
- How multicollinearity is an issue and what to do to fix it  
- PCA for dimensionality reduction  
- k-means for clustering  
- How to validate results from both analysis  

Thanks for attending!  




