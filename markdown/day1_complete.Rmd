---
title: "day1_complete"
output: html_document
---

# 1. Setup  
```{r Installing and loading packages, warning=F}
#install.packages("easypackages")
library(easypackages)

packages("janitor") # clean column names
packages("tidyverse") # wrangling and general plotting
packages("ggcorrplot") # exploratory correlation plot
packages("broom") # extracting model info as data frame
packages("car") # vif
packages("factoextra") # multivariate plotting

# Setting plotting theme
theme_set(theme_bw()+
            theme(panel.background = element_rect(fill="gray80"),
                  panel.grid = element_blank(),
                  strip.text = element_text(face = "bold", size=10)))


```

```{r mvadf}
mvadf <- read_csv("../data/data_R_MVA.csv")
```

# SLIDES - What is multivariate analysis?  

Most common: multiple Xs (predictors)  
    y ~ x1 + x2 + .... xn
    yield ~ precip + temp + som + hybrid
    
Perhaps multiple Xs (predictors) without a Y (outcome)
    ~ x1 + x2 + .... xn

While some can have multiple Ys (outcomes)
  y1 + y2 ~ x1 + x2 (MANOVA)
  yield + biomass ~ precip + temp  
  
# SLIDES - When to use multivariate?  
When multiple predictors can be used to explain an outcome.  
When multiple predictors can be used to create data-driven groups (clusters).  

Commonly not a "designed experiment" analysis, but an exploratory relationship approach.

# SLIDES - Main types of multivariate  
- Dimensionality reduction  
    - **Principal component analysis (PCA)**
    - Principal coordinate analysis (PCoA)
    - Factor analysis
    
- Clustering 
    - **k-means**, c-means, x-means  
    - **Non-numerical multidimensional scaling**
    - nearest neighbor
  
- Relationship between variables / Prediction  
  - Multiple linear regression/stepwise regression
  - **Random forest**
  - **Conditional inference tree**

# SLIDES - A few ways to characterize multivariate analysis  
- Based on analysis goal:
    - Decrease number of dimensions  
    - Create clusters  
    - Establish significant relationships among multiple predictors and an outcome.  

- Based on Y (outcome) existence:
    - Supervised
    - Unsupervised  

- If Y exists, based on its type:
    - Categorical: classification
    - Numerical: regression

# 2. Wrangling  
```{r print mvadf}
mvadf
```
The data set contains categorical and numerical columns.

```{r mvadf summary}
mvadf %>%
  summary()
```
Let's transform character variables into factor, 

AND

select some numerical variables to continue our analysis (precip, temperature, radiation, and vapor pressure deficit).  

```{r mvadf_w}
mvadf_w <- mvadf %>%
  clean_names() %>%
  mutate(across(where(is.character), factor)) %>%
  dplyr::select(gy:sand, starts_with(c("pp_","tm_","rad_","vpd_")))

mvadf_w
```
# 3. EDA  
How are variables related? Any strong correlations that we should watch out for?  
```{r correlation matrix}
# Estimating significance
p.mat <-  mvadf_w %>%
  dplyr::select(where(is.numeric)) %>%
  cor_pmat()

mvadf_w %>%
  dplyr::select(where(is.numeric)) %>%
  cor() %>%
  ggcorrplot(hc.order = TRUE,
             type = "lower", 
             p.mat = p.mat, 
             insig = "blank",
             lab= TRUE)+
  ggsave("../output/cormat.png", width = 10, height = 10)


```

How do variables relate to grain yield in a bivariate relationship?  
```{r eda2}
mvadf_w %>%
  dplyr::select(where(is.numeric)) %>%
  pivot_longer(cols=!gy) %>%
  ggplot(aes(x=value, y=gy))+
  geom_point(shape=21, alpha=.7, fill="purple")+
  geom_smooth(method="lm", se=F, color="yellow", size=3)+
  facet_wrap(~name, scales = "free_x", ncol=6)+
  ggsave("../output/gyvsall.png", width = 15, height = 15)



```

# 4. Multicollinearity 
Multicollinearity definition: more than two explanatory variables in a multiple regression model are highly linearly related.


Multicollinearity is an issue because:  
  - Model estimates magnitude and direction (+ or -) can change for multicollinear variables compared to a non-multicollinear model.  
  
  - Model estimates standard error are inflated, directly affecting p-values, estimate significance, and power.

Let's select a few variables to run some tests.  
Two uncorrelated variables:
```{r uncorrelated}
mvadf_w %>%
  ggplot(aes(x=vpd_pre, y=rad_2))+
  geom_point()+
  geom_smooth(method="lm")
```

Two correlated variables:
```{r correlated}
mvadf_w %>%
  ggplot(aes(x=vpd_pre, y=tm_pre))+
  geom_point()+
  geom_smooth(method="lm")

```

Now let's fit some models with one or two uncorrelated and correlated variables explaining yield and see what happens.
```{r gy ~ vpd_pr}
lm_vpd <- lm(gy ~ vpd_pre,
   data=mvadf_w) %>%
  tidy() %>%
  filter(term!="(Intercept)") %>%
  mutate(mod="lm_vpd")

lm_vpd

```

```{r gy ~ tm_pre}
lm_tm <- lm(gy ~ tm_pre,
   data=mvadf_w) %>%
  tidy() %>%
  filter(term!="(Intercept)") %>%
  mutate(mod="lm_tm")

lm_tm
```

```{r gy ~ vpd_pre + rad_2}
lm_vpd.rad <- lm(gy ~ vpd_pre + rad_2,
   data=mvadf_w) %>%
  tidy() %>%
  filter(term!="(Intercept)") %>%
  mutate(mod="lm_2var.uncor")

lm_vpd.rad

```

```{r gy ~ vpd_pre + tm_pre}
lm_vpd.tm <- lm(gy ~ vpd_pre + tm_pre,
   data=mvadf_w) %>%
  tidy() %>%
  filter(term!="(Intercept)") %>%
  mutate(mod="lm_2var.cor")

lm_vpd.tm

```

```{r Checking multicollinearity}
lm_vpd %>%
  bind_rows(lm_tm, 
            lm_vpd.rad,
            lm_vpd.tm) %>%
  mutate(mod=factor(mod,
                    levels=c("lm_vpd",
                             "lm_tm",
                             "lm_2var.uncor",
                             "lm_2var.cor"))) %>%
    filter(term!="rad_2") %>%
  ggplot(aes(x=mod))+
  geom_pointrange(aes(y=estimate,
    ymin=estimate-std.error,
                      ymax=estimate+std.error))+
  scale_x_discrete(guide = guide_axis(n.dodge = 2))+
  facet_wrap(~term, scales = "free_y")

```

What has happened with tm_pre and vpd_pre estimates and standard error when modeled i) alone, or with another uncorrelated variable vs. ii) with another correlated variable?

Let's check the variance inflation factor (VIF) of both uncorrelated and correlated models
```{r vif}
# Uncorrelated
vif(lm(gy ~ vpd_pre + rad_2,
   data=mvadf_w)) 

# Correlated
vif(lm(gy ~ vpd_pre + tm_pre,
   data=mvadf_w))

```
VIF values range from 1 to positive infinite.
General rule of thumb:
    - VIF ~ 1: no multicollinearity
    - VIF between 1 and 5: moderate multicollinearity
    - VIF > 5: severe multicollinearity

# SLIDES - Dealing with multicollinearity  
So what now?  
How can we deal with correlated variables in a multivariate approach?  
Options:  
    - Dimensionality reduction  
        - By hand  
        - By math  
  
    - Algorithm that handles multicollinearity
        - Variable selection  
        - Multivariate by ensembling multiple bivariates  

Many multivariate approaches deal with some sort of similarity/dissimilarity measure among predictors.  

In those cases, predictors with vastly different scales (e.g. SOM from 0 to 7 vs precipitation from 0 to 300) need to be standardized so measurement scale does not affect variable importance.    

Thus, our numerical predictor variables need to be normalized (center and scale) before starting our multivariate analysis. 

We can normalize the data in the same step of calculating a distance matrix.  
```{r selecting only numerical vars}
mvadf_wn <- mvadf_w %>%
  dplyr::select(where(is.numeric), -gy) 

mvadf_wn
```

# 5. PCA  
PCA is a dimensionality reduction approach that accomodates only numerical variables. 

Finds linear relationships among predictors that can be represented in a lower number of uncorrelated dimensions.

Works well when at least some predictors are correlated.  

PCA:
- Used for dimensionality reduction 
- Is an unsupervised analysis (no outcome)
- Only takes predictors
- Predictors need to be numerical

```{r pca model}
mod_pca <- prcomp(mvadf_wn, scale. = T) 

summary(mod_pca)
```

```{r pca checking number of components}
# Scree plot
fviz_eig(mod_pca,
         addlabels=TRUE)

```
PCs 1 and 2 explain ~27% and ~14% (41%) of total variance.
Not great, indicates that original variables were not as highly correlated.


If wanted to use enough PCs to explain 60% of total variance, how many would we need?
```{r PCs to explain 60% variance}
mod_pca %>%
  get_eig() %>%
  mutate(pc=1:nrow(.)) %>%
  ggplot(aes(x=pc, y=cumulative.variance.percent))+
  geom_bar(stat="identity")+
  geom_hline(yintercept = 60)


```
We would need 5 PCs.
Normally we wish to use 2-3 PCs, but 5 is certainly better than 24 original variables.  

Let's inspect PC1.
What are the weights that each variable received in this PC?
```{r PC1 weights}
mod_pca$rotation %>%
  as.data.frame() %>%
  rownames_to_column(var = "var") %>%
  ggplot(aes(x=reorder(var,desc(PC1)), y=PC1))+
  geom_bar(stat="identity", aes(fill=PC1), show.legend = F)+
  scale_fill_gradient(low = "red", high = "blue")+
  theme(axis.text.x = element_text(angle=45, hjust=1))

```

Which variables contributed most to PC1, regardless of direction?
```{r variables contributing to PC1}
fviz_contrib(mod_pca, choice = "var", axes = 1)
```
Let's make a biplot with both PCs 1 and 2 variables:
```{r pca variable contribution }
fviz_pca_var(mod_pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )

```

Let's plot PC1 vs PC2 scores, look for any groupings.
```{r PC1 vs PC2}
fviz_pca_ind(mod_pca)

```
No clear groups.

What if we draw elipses by fungicide levels?
```{r PC1 vs PC2 by fungicide}
fviz_pca_ind(mod_pca,
             col.ind = factor(mvadf_w$fung),
             addEllipses = TRUE)

```
What did we learn?
- Original variables not strongly correlated
- Need at least 5 PCs to cover ~60% of original variables variance
- Most important variables in PC1 were related to vpd and temperature.

What now?
Let's add the first 5 PCs to our original dataset and run a regression.
```{r pca post-anova}
# Extract first 5 PCs scores
pca_scores <- mod_pca$x %>%
  as.data.frame() %>%
  dplyr::select(PC1:PC5)

pca_scores

# Adding PCs 1-5 scores to original data set
mvadf_wpostpca <- mvadf_w %>%
  bind_cols(pca_scores)

# Regression of yield ~ PCs

lm_pca <- lm(gy ~ PC1 + PC2 + PC3 + PC4 + PC5,
             data=mvadf_wpostpca)

# Summary  
summary(lm_pca)

# Checking VIF
vif(lm_pca)

# Plotting yield vs PC1
ggplot(mvadf_wpostpca, aes(x=PC1, y=gy))+
  geom_point()+
  geom_smooth(method="lm")

# Plotting yield vs PC3
ggplot(mvadf_wpostpca, aes(x=PC3, y=gy))+
  geom_point()+
  geom_smooth(method="lm")

```

Only PCs 1, 3, and 5 explained yield (look at which variables were most important to each PC for interpretation).

As expected, PCs were not multicollinear (VIF=1).

# 6. k-means  
k-means:
- Used for clustering
- Is an unsupervised analysis (no outcome)
- Only takes predictors
- Predictors need to be numerical

```{r}
knitr::include_graphics("https://miro.medium.com/max/960/1*KrcZK0xYgTa4qFrVr0fO2w.gif")

```




